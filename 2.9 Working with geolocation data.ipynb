{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with geolocation data\n",
    "\n",
    "\n",
    "In this practical, we will look at the location data we have stored for each tweet.  We will learn how to process geolocation coordinates, and obtain a more human readable label for these locations by obtaining full address information such as town, city, and county names, which we will be using to summarise and visualise the tweets.\n",
    "\n",
    "Our first task this week is to load our data set and beginning exploring the locations referenced in the Twitter data set.  We import the Pandas library as before, read in the .csv file, and call .head(20) on the data frame to view the first 20 rows of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>_id</th>\n",
       "      <th>place_name</th>\n",
       "      <th>place_type</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Tue Sep 29 20:57:35 +0000 2020</td>\n",
       "      <td>1311047494021701632</td>\n",
       "      <td>Oadby</td>\n",
       "      <td>city</td>\n",
       "      <td>52.5883</td>\n",
       "      <td>-1.09612</td>\n",
       "      <td>\"Hovis granary wholemeal\" in #Leicester #Unite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Tue Sep 29 20:43:53 +0000 2020</td>\n",
       "      <td>1311044046471585792</td>\n",
       "      <td>Oadby</td>\n",
       "      <td>city</td>\n",
       "      <td>52.5883</td>\n",
       "      <td>-1.09612</td>\n",
       "      <td>\"Celery\" in #Leicester #UnitedKingdom https://...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Tue Sep 29 20:43:26 +0000 2020</td>\n",
       "      <td>1311043935322611712</td>\n",
       "      <td>Hammersmith</td>\n",
       "      <td>city</td>\n",
       "      <td>51.4865</td>\n",
       "      <td>-0.22651</td>\n",
       "      <td>\"Eat 17 - small baguettes\" in #London #UnitedK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Tue Sep 29 20:41:30 +0000 2020</td>\n",
       "      <td>1311043448972083203</td>\n",
       "      <td>Barking</td>\n",
       "      <td>city</td>\n",
       "      <td>51.5409</td>\n",
       "      <td>0.12200</td>\n",
       "      <td>\"Salad\" in #Dagenham #UnitedKingdom https://t....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Tue Sep 29 20:41:06 +0000 2020</td>\n",
       "      <td>1311043346257772544</td>\n",
       "      <td>Glasgow</td>\n",
       "      <td>city</td>\n",
       "      <td>55.8720</td>\n",
       "      <td>-4.26896</td>\n",
       "      <td>\"Humous and Chipotle Wrap\" in #Glasgow #United...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             date                  _id   place_name  \\\n",
       "0  Tue Sep 29 20:57:35 +0000 2020  1311047494021701632        Oadby   \n",
       "1  Tue Sep 29 20:43:53 +0000 2020  1311044046471585792        Oadby   \n",
       "2  Tue Sep 29 20:43:26 +0000 2020  1311043935322611712  Hammersmith   \n",
       "3  Tue Sep 29 20:41:30 +0000 2020  1311043448972083203      Barking   \n",
       "4  Tue Sep 29 20:41:06 +0000 2020  1311043346257772544      Glasgow   \n",
       "\n",
       "  place_type      lat      lng  \\\n",
       "0       city  52.5883 -1.09612   \n",
       "1       city  52.5883 -1.09612   \n",
       "2       city  51.4865 -0.22651   \n",
       "3       city  51.5409  0.12200   \n",
       "4       city  55.8720 -4.26896   \n",
       "\n",
       "                                                text  \n",
       "0  \"Hovis granary wholemeal\" in #Leicester #Unite...  \n",
       "1  \"Celery\" in #Leicester #UnitedKingdom https://...  \n",
       "2  \"Eat 17 - small baguettes\" in #London #UnitedK...  \n",
       "3  \"Salad\" in #Dagenham #UnitedKingdom https://t....  \n",
       "4  \"Humous and Chipotle Wrap\" in #Glasgow #United...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/BBKdatasciencetaster/DS/main/data/twitter_data_olio_UK_b.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to look at summary statistics computed over the whole dataset, the .describe() method produces a simple report on the number of rows of data, the number of unique items we have, and the most common category together with its count. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count         4634\n",
       "unique         289\n",
       "top       East Ham\n",
       "freq           241\n",
       "Name: place_name, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['place_name'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From running the example, we see that the dataset contains 4,634 unique tweets, with 289 unique locations mentioned across the United Kingdom. Of the total tweets the most mentioned location is 'East Ham' with 241 tweets.  To learn more about the whole data set, we can summarise the values of our columns. We will first explore the locations that are mentioned with the tweets, by grouping the data according to the locations stored under the 'place_name' column. We then select the top-10 rows with the highest frequency counts as our summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "place_name\n",
       "East Ham       241\n",
       "Walthamstow    226\n",
       "Richmond       188\n",
       "Reading        176\n",
       "Hull           148\n",
       "Islington      145\n",
       "Stevenage      129\n",
       "Grays          114\n",
       "Ipswich        108\n",
       "Poplar         108\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "place_names = df.groupby(['place_name']).size().nlargest(10)\n",
    "place_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise these summary statistics to get a feel for the proportion of tweets mentioning each location. We plot our summary as a simple bar chart, and add a title and y-axis label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x15911934cf8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "barchart = place_names.plot(kind='bar')\n",
    "barchart.set_title('Tweeted locations')\n",
    "barchart.set_ylabel('Count')\n",
    "barchart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another column we can inspect is the 'place_type', which will tell us the types of locations mentioned, such as 'city'. Calling the .size() method on the grouped data will return the number of rows per categ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "place_type\n",
       "admin      83\n",
       "city     4551\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['place_type']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of our data set contains locations relating to cities. However, a small proportion of our data (83 rows in total) are described by the Twitter API as 'admin'.  Let's explore this further to see what this means by selecting the subset of our data where the 'place_type' is equal to 'admin'. We will select a random sample of 20 rows from this subset to act as our summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>_id</th>\n",
       "      <th>place_name</th>\n",
       "      <th>place_type</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2951</td>\n",
       "      <td>Wed Sep 30 07:00:34 +0000 2020</td>\n",
       "      <td>1311199239511461888</td>\n",
       "      <td>East</td>\n",
       "      <td>admin</td>\n",
       "      <td>52.334200</td>\n",
       "      <td>-0.213470</td>\n",
       "      <td>\"Shower foam \" in #Huntingdon #UnitedKingdom h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>952</td>\n",
       "      <td>Tue Sep 29 10:29:04 +0000 2020</td>\n",
       "      <td>1310889323114397696</td>\n",
       "      <td>South East</td>\n",
       "      <td>admin</td>\n",
       "      <td>51.600193</td>\n",
       "      <td>-1.263759</td>\n",
       "      <td>\"Wholemeal 400g \" in #Didcot #UnitedKingdom ht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>963</td>\n",
       "      <td>Tue Sep 29 10:20:24 +0000 2020</td>\n",
       "      <td>1310887144135766016</td>\n",
       "      <td>South East</td>\n",
       "      <td>admin</td>\n",
       "      <td>51.600193</td>\n",
       "      <td>-1.263759</td>\n",
       "      <td>\"White thick toaste bread 800g\" in #Didcot #Un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2542</td>\n",
       "      <td>Mon Sep 28 08:46:52 +0000 2020</td>\n",
       "      <td>1310501214203904003</td>\n",
       "      <td>North East</td>\n",
       "      <td>admin</td>\n",
       "      <td>54.983900</td>\n",
       "      <td>-1.759730</td>\n",
       "      <td>\"Pret a Manger mixed bakery goods\" in #Newcast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>Tue Sep 29 10:00:38 +0000 2020</td>\n",
       "      <td>1310882169938731010</td>\n",
       "      <td>South East</td>\n",
       "      <td>admin</td>\n",
       "      <td>51.600193</td>\n",
       "      <td>-1.263759</td>\n",
       "      <td>\"Fair trade bananas \" in #Didcot #UnitedKingdo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2477</td>\n",
       "      <td>Mon Sep 28 09:56:11 +0000 2020</td>\n",
       "      <td>1310518660436578304</td>\n",
       "      <td>Bailiwick of Jersey</td>\n",
       "      <td>admin</td>\n",
       "      <td>49.201000</td>\n",
       "      <td>-2.143830</td>\n",
       "      <td>\"Hovis white medium loads \" in #Jersey #United...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>951</td>\n",
       "      <td>Tue Sep 29 10:30:03 +0000 2020</td>\n",
       "      <td>1310889569005568000</td>\n",
       "      <td>South East</td>\n",
       "      <td>admin</td>\n",
       "      <td>51.600193</td>\n",
       "      <td>-1.263759</td>\n",
       "      <td>\"No crust 50/50 400g \" in #Didcot #UnitedKingd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3361</td>\n",
       "      <td>Thu Oct 01 08:11:11 +0000 2020</td>\n",
       "      <td>1311579398668275718</td>\n",
       "      <td>West Midlands</td>\n",
       "      <td>admin</td>\n",
       "      <td>52.366900</td>\n",
       "      <td>-1.701310</td>\n",
       "      <td>\"steamed beetroot in vinegar \" in #Solihull #U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3411</td>\n",
       "      <td>Thu Oct 01 07:43:15 +0000 2020</td>\n",
       "      <td>1311572369148968962</td>\n",
       "      <td>South East</td>\n",
       "      <td>admin</td>\n",
       "      <td>51.454500</td>\n",
       "      <td>-0.856300</td>\n",
       "      <td>\"Broccoli florets\" in #Reading #UnitedKingdom ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>Tue Sep 29 09:35:07 +0000 2020</td>\n",
       "      <td>1310875745548472320</td>\n",
       "      <td>Bailiwick of Jersey</td>\n",
       "      <td>admin</td>\n",
       "      <td>49.201000</td>\n",
       "      <td>-2.143830</td>\n",
       "      <td>\"Aubergine\" in #Jersey #UnitedKingdom https://...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                date                  _id  \\\n",
       "2951  Wed Sep 30 07:00:34 +0000 2020  1311199239511461888   \n",
       "952   Tue Sep 29 10:29:04 +0000 2020  1310889323114397696   \n",
       "963   Tue Sep 29 10:20:24 +0000 2020  1310887144135766016   \n",
       "2542  Mon Sep 28 08:46:52 +0000 2020  1310501214203904003   \n",
       "1020  Tue Sep 29 10:00:38 +0000 2020  1310882169938731010   \n",
       "2477  Mon Sep 28 09:56:11 +0000 2020  1310518660436578304   \n",
       "951   Tue Sep 29 10:30:03 +0000 2020  1310889569005568000   \n",
       "3361  Thu Oct 01 08:11:11 +0000 2020  1311579398668275718   \n",
       "3411  Thu Oct 01 07:43:15 +0000 2020  1311572369148968962   \n",
       "1070  Tue Sep 29 09:35:07 +0000 2020  1310875745548472320   \n",
       "\n",
       "               place_name place_type        lat       lng  \\\n",
       "2951                 East      admin  52.334200 -0.213470   \n",
       "952            South East      admin  51.600193 -1.263759   \n",
       "963            South East      admin  51.600193 -1.263759   \n",
       "2542           North East      admin  54.983900 -1.759730   \n",
       "1020           South East      admin  51.600193 -1.263759   \n",
       "2477  Bailiwick of Jersey      admin  49.201000 -2.143830   \n",
       "951            South East      admin  51.600193 -1.263759   \n",
       "3361        West Midlands      admin  52.366900 -1.701310   \n",
       "3411           South East      admin  51.454500 -0.856300   \n",
       "1070  Bailiwick of Jersey      admin  49.201000 -2.143830   \n",
       "\n",
       "                                                   text  \n",
       "2951  \"Shower foam \" in #Huntingdon #UnitedKingdom h...  \n",
       "952   \"Wholemeal 400g \" in #Didcot #UnitedKingdom ht...  \n",
       "963   \"White thick toaste bread 800g\" in #Didcot #Un...  \n",
       "2542  \"Pret a Manger mixed bakery goods\" in #Newcast...  \n",
       "1020  \"Fair trade bananas \" in #Didcot #UnitedKingdo...  \n",
       "2477  \"Hovis white medium loads \" in #Jersey #United...  \n",
       "951   \"No crust 50/50 400g \" in #Didcot #UnitedKingd...  \n",
       "3361  \"steamed beetroot in vinegar \" in #Solihull #U...  \n",
       "3411  \"Broccoli florets\" in #Reading #UnitedKingdom ...  \n",
       "1070  \"Aubergine\" in #Jersey #UnitedKingdom https://...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['place_type'] == 'admin'].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the summary we observe that the place type 'admin' appears to be assigned to large regions, in other words, the location encoded by the tweet relates to an administrative district rather than a city, despite the presence of hashtags relating to the city name in the tweet text, e.g. #Falkirk, #Didcot, and #Enfield. Before we continue we will correct the place name for these tweets so that they refer to the city rather than the level of administrative district."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How might we correct these values?  If we think about the information we have about each tweet, can you think of a method to retrieve the correct city name? We will correct this issue by extracting the missing values using another API that will identify a location and the full address given coordinates in the form of latitude and longitude, by applying a method known as reverse geo-coding. Given a set of coordinates, reverse geo-coding involves extracting a text string with address information that we can use to fill in our missing data. We will extract the town or city name for the locations listed as \"admin\" using the geo-coordinates latitude and longitude stored in the tweet.  \n",
    "\n",
    "To do this we will import another Python library called geopy, which will take these and return a full address. You can learn more about using the geopy library by reading the official documentation (https://geopy.readthedocs.io/en/latest/). \n",
    "\n",
    "First, let's see how this works in practice. We first import the necessary tools and define a random name for our application, which will be used by the API to identify our request. This identifier can be any text string you can think of.  \n",
    "\n",
    "Once the library is imported, we create a 'geolocator' object, which has a 'reverse' method where we pass the latitude and longitude coordinates. This method returns address details about the location, which we will store in a variable called 'location': "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse geocoding\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "app_name = \"my awesome app\" # <- necessary to identify your request and access the geopy API.\n",
    "\n",
    "geolocator = Nominatim(user_agent=app_name)\n",
    "\n",
    "coordinates = '51.52194, -0.13032'\n",
    "\n",
    "location = geolocator.reverse(coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now inspect the data returned by the method call. To view a text string of the full address, we can view the .address attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Birkbeck, University of London, Byng Place, St Pancras, London Borough of Camden, London, Greater London, England, WC1E 7HY, United Kingdom'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieving the full address string:\n",
    "location.address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also obtain a more useful representation that we can use to extract specific values from the address.  The returned location data also has a 'raw' attribute containing a JSON representation of the address and additional metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'place_id': 295556486,\n",
       " 'licence': 'Data © OpenStreetMap contributors, ODbL 1.0. https://osm.org/copyright',\n",
       " 'osm_type': 'relation',\n",
       " 'osm_id': 11943583,\n",
       " 'lat': '51.522071100000005',\n",
       " 'lon': '-0.13046659668430677',\n",
       " 'display_name': 'Birkbeck, University of London, Byng Place, St Pancras, London Borough of Camden, London, Greater London, England, WC1E 7HY, United Kingdom',\n",
       " 'address': {'amenity': 'Birkbeck, University of London',\n",
       "  'road': 'Byng Place',\n",
       "  'suburb': 'St Pancras',\n",
       "  'city_district': 'London Borough of Camden',\n",
       "  'city': 'London',\n",
       "  'state_district': 'Greater London',\n",
       "  'state': 'England',\n",
       "  'postcode': 'WC1E 7HY',\n",
       "  'country': 'United Kingdom',\n",
       "  'country_code': 'gb'},\n",
       " 'boundingbox': ['51.5214861', '51.5232251', '-0.1310916', '-0.1295004']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieving a JSON object for querying specific attributes\n",
    "location.raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Locate yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the .address attribute stores the address according to various fields, such as 'road', 'suburb', and 'city', which means we have all the information we need to address the issue of missing or incorrect values.  Let's check the address information for your current location.  First, visit <a src=\"https://mylocation.org/\">mylocation.org</a> and make a note of the latitute and longitude coordinates. Paste these together and assign them to the 'coordinates' variable.  When you run the example again, you should see the full address of your current location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't forget, your coordinates should be formatted to look like this '3.0000,50.0000' \n"
     ]
    }
   ],
   "source": [
    "# reverse geocoding\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "app_name = \"my awesome app\" # <- necessary to identify your request and access the geopy API.\n",
    "\n",
    "geolocator = Nominatim(user_agent=app_name)\n",
    "\n",
    "coordinates = 'YOUR LATITUDE, YOUR LONGITUDE' # <- paste your coordinates here\n",
    "try:\n",
    "    location = geolocator.reverse(coordinates)\n",
    "    location\n",
    "except: print(\"Don't forget, your coordinates should be formatted to look like this '3.0000,50.0000' \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's return to the issue we wish to address, that is we want to extract the county name for areas defined as administrative ('admin'). One cavaet is that not all the missing values are of the type 'city', some locations maybe 'suburbs', or 'villages', so we need to capture this in our code to ensure that we fill in all the missing data correctly.  We first load the data we processed earlier, then proceed to iterate through each row and pull out the relevant values based on the conditions we have set, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reverse geo-coding in progress, this may take a minute...\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Reverse geo-coding in progress, this may take a minute...\")\n",
    "\n",
    "# Import the reverse geocoding python library\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# necessary to identify your request and gain access to the geopy API.\n",
    "app_name = \"my awesome app\"\n",
    "\n",
    "geolocator = Nominatim(user_agent=app_name)\n",
    "\n",
    "for idx, lat, lng, place_type, text in zip(df.index, df.lat, df.lng, df.place_type, df.text):\n",
    "    # skip all rows if the place_type is different from admin.\n",
    "    if place_type != 'admin': continue \n",
    "        \n",
    "    location = geolocator.reverse(\"%s, %s\" %(lat, lng))\n",
    "    address = location.raw['address']\n",
    "    \n",
    "    # Check to see if there is a 'city' attribute we can access.\n",
    "    place_name = address.get('city', None)\n",
    "    place_type = 'city' # <- we will assume a default place type of city.\n",
    "    \n",
    "    if place_name == None:\n",
    "        # No city attribute, check to see if there is a 'suburb' attribute we can access.\n",
    "        place_name = address.get('suburb', None) \n",
    "        place_type = 'suburb'\n",
    "        \n",
    "    if place_name == None:\n",
    "        # No city or suburb attribute, check to see if there is a 'hamlet' attribute we can access.\n",
    "        place_name = address.get('hamlet', None) \n",
    "        place_type = 'hamlet'   \n",
    "        \n",
    "    if place_name == None: \n",
    "        # No city, suburb, or hamlet attribute, check to see if there is a 'town' attribute we can access.\n",
    "        place_name = address.get('town', None) \n",
    "        place_type = 'town'\n",
    "        \n",
    "    if place_name == None: \n",
    "        # None of the above, check to see if there is a 'village' attribute we can access.\n",
    "        place_name = address.get('village', None)\n",
    "        place_type = 'village'\n",
    "    \n",
    "    df.loc[idx, 'place_name'] = place_name\n",
    "    df.loc[idx, 'place_type'] = place_type\n",
    "\n",
    "    #print([idx], (lat, lng), place_name, place_type)\n",
    "print (\"All done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once complete, we can now inspect the unique values stored in the place_types column to verify that all rows with a place type of 'admin' have been replaced with more specific information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['city', 'suburb', 'village', 'hamlet'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['place_type'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see all of the locations listed as 'admin' by the Twitter API, have now been corrected using information we have gathered through reverse geo-coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
